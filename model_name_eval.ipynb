{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ed0e0-36a0-490a-b217-fe60a7422f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "f='./depmap/Model.csv.gz'\n",
    "host='127.0.0.1'\n",
    "port='9999'\n",
    "model='meta-llama/Meta-Llama-3-70B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = 'cmsc-35360'\n",
    "openai_api_base = f\"http://{host}:{port}/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(f):\n",
    "    df=pd.read_csv(f)\n",
    "    print(f'done importing dataframe (rows, columns) = {df.shape}.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b368e7-4869-4bd8-925e-daa395549f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_random_element_not_equal_to(array, correct):\n",
    "    '''\n",
    "    Given an array of unique answers, select an answer not\n",
    "    equal to the correct answer.\n",
    "    '''\n",
    "    if len(array) == 0:\n",
    "        raise ValueError(\"The array is empty\")\n",
    "\n",
    "    # Filter out elements equal to `correct`\n",
    "    filtered_array = [element for element in array if element != correct]\n",
    "    \n",
    "    if not filtered_array:\n",
    "        raise ValueError(\"No valid elements to choose from\")\n",
    "\n",
    "    # Select a random element from the filtered list\n",
    "    return random.choice(filtered_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70546a55-6269-4c69-a75c-208ad0af1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_question(df, index, array):\n",
    "    '''\n",
    "    Construct a multiple choice question from df using row at index.\n",
    "    '''\n",
    "    if index < 0 or index >= len(df):\n",
    "        raise IndexError(\"Index out of bounds\")\n",
    "    \n",
    "    cell_line_name = df.iloc[index]['CellLineName']\n",
    "    correct_answer = df.iloc[index]['OncotreePrimaryDisease']\n",
    "\n",
    "    wrong_answer_1 = select_random_element_not_equal_to(array, correct_answer)\n",
    "    wrong_answer_2 = select_random_element_not_equal_to(array, correct_answer)\n",
    "    wrong_answer_3 = select_random_element_not_equal_to(array, correct_answer)\n",
    "\n",
    "    while wrong_answer_1 == wrong_answer_2 or wrong_answer_1 == wrong_answer_3 or wrong_answer_2 == wrong_answer_3:\n",
    "        wrong_answer_1 = select_random_element_not_equal_to(array, correct_answer)\n",
    "        wrong_answer_2 = select_random_element_not_equal_to(array, correct_answer)\n",
    "        wrong_answer_3 = select_random_element_not_equal_to(array, correct_answer)\n",
    "        \n",
    "    answers = [correct_answer,\n",
    "               wrong_answer_1,\n",
    "               wrong_answer_2,\n",
    "               wrong_answer_3]\n",
    "    random.shuffle(answers)\n",
    "\n",
    "    d = {\"a\": answers[0],\n",
    "         \"b\": answers[1],\n",
    "         \"c\": answers[2],\n",
    "         \"d\": answers[3],\n",
    "        }\n",
    "\n",
    "    for choice in d.keys():\n",
    "        if d[choice] == correct_answer:\n",
    "            correct_choice = choice\n",
    "    \n",
    "    # question = f'''The tumor cell line named {cell_line_name} is\n",
    "    # question = f'''The tumor cell line named {cell_line_name} is a biological model for which primary disease?\n",
    "    question = f'''The cell line named {cell_line_name} is a biological model for which primary disease?\n",
    "\n",
    "a) {d['a']},\n",
    "b) {d['b']},\n",
    "c) {d['c']},\n",
    "d) {d['d']}.'''\n",
    "    \n",
    "    return question, correct_choice, correct_answer, cell_line_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54595d-5113-40f4-bfd4-1fdc7575007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "df = load_data(f)\n",
    "array = df['OncotreePrimaryDisease'].unique()\n",
    "\n",
    "\n",
    "\n",
    "sys_reg_prompt = '''You are a cancer research scientist studying the potential effects of various small molecules, peptides, and \n",
    "antibiodies on tumor cell growth. You will be presented with a series of multiple choice questions. Please select the correct\n",
    "choice. Return the answer in json format {\"CHOICE\": choice, \"ANSWER\": answer} where choice is only the alphabetic character \n",
    "associated with the full answer.'''\n",
    "\n",
    "# This is not working yet because the experts talk to each other thus making the\n",
    "# parsing of the final answer different from the sys_reg_prompt.\n",
    "sys_tot_prompt = '''Imagine three experts are answering this question.\n",
    "They will brainstorm the answer step by step, reasoning carefully and taking all facts into consideration.\n",
    "All experts will write down one step of their thinking, then share it with the group.\n",
    "They will each critique their response and all the responses of others.\n",
    "They will check their answer based on science, laws of physics and logic.\n",
    "Then all experts will go on to the next step and write down this step of their thinking.\n",
    "They will keep going through steps until they reach their conclusions taking into account the thoughts of the other experts.\n",
    "If at anytime they realize that there is a flaw in their logic they will backtrack to where the flaw occurred.\n",
    "If any expert realizes they are wrong at any point they acknowledge this and start another train of thought.\n",
    "Each expert will assign a likelihood of their current assertion being correct.\n",
    "Continue until the experts agree on the single most likely choice. Return the response in json format FINAL_ANSWER={\"DISCUSSION\": discussion, \"CHOICE\": choice, \"ANSWER\": answer} where choice is only the alphabetic character \n",
    "associated with the full answer.'''\n",
    " \n",
    "for i in range(0, 9):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    responses = []\n",
    "\n",
    "\n",
    "    for i in tqdm( range(df.shape[0]) ):\n",
    "        # get question, correct choice and answer\n",
    "        question, correct_choice, correct_answer, cell_line_name = construct_question(df, i, array)\n",
    "    \n",
    "        # construct message\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": sys_reg_prompt\n",
    "            },\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "        chat_response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            # logprobs=1,\n",
    "            # top_logprobs=1,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=2560,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            response = json.loads(chat_response.choices[0].message.content)\n",
    "            response['CORRECT CHOICE'] = correct_choice\n",
    "            response['CORRECT ANSWER'] = correct_answer\n",
    "            response['CELL_LINE_NAME'] = cell_line_name\n",
    "    \n",
    "            if response['CHOICE'] == correct_choice:\n",
    "                response['SCORE'] = 1\n",
    "                num_correct = num_correct + 1\n",
    "            else:\n",
    "                response['SCORE'] = 0\n",
    "    \n",
    "            total = total + 1\n",
    "            responses.append(response)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            print(f\"{question}\")\n",
    "            print(f\"{chat_response.choices[0].message.content}\")\n",
    "\n",
    "            response[\"CHOICE\"] =  \"e\"\n",
    "            response[\"ANSWER\"] =  chat_response.choices[0].message.content\n",
    "            response['CORRECT CHOICE'] = correct_choice\n",
    "            response['CORRECT ANSWER'] = correct_answer\n",
    "            response['CELL_LINE_NAME'] = cell_line_name\n",
    "            response['SCORE'] = 0\n",
    "            \n",
    "            responses.append(response)\n",
    "            pass\n",
    "    \n",
    "    print(f'{num_correct} correct responses out of {total}')\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    response_df = pd.DataFrame(responses)\n",
    "    response_df.to_csv(f'model_name_eval_{timestamp}.tsv', index=None, sep=\"\\t\")\n",
    "\n",
    "    with open('model_name_eval_summary.txt', 'a') as f:\n",
    "        print(f'{timestamp}\\t{num_correct} correct responses out of {total}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ec0dd-6877-43b8-8b05-3fb030868bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
